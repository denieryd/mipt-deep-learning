{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUWCAY5opP87"
   },
   "source": [
    "<p style=\"align: center;\"><img align=center src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\"  width=500 height=400></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEkVD5qHpP89"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wj5MrpmRpP89"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Элементы теории оптимизации. Производные и частные производные.</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1xIGzO5pP8-"
   },
   "source": [
    "<p style=\"text-align: center;\">(На основе https://github.com/romasoletskyi/Machine-Learning-Course)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODaZDX75pP8-"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение линейной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tldAx431pP8_"
   },
   "source": [
    "Давайте рассмотрим линейную функцию $y=kx+b$ и построим график: <br>  \n",
    "\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/c/c1/Wiki_slope_in_2d.svg) <br>  \n",
    "\n",
    "Введём понятие **приращения** функции в точке $(x, y)$ как отношение вертикального изменения (измненеия функции по вертикали) $\\Delta y$ к горизонтальному изменению $\\Delta x$ и вычислим приращение для линейной функции:  \n",
    "\n",
    "$$приращение (\"slope\")=\\frac{\\Delta y}{\\Delta x}=\\frac{y_2-y_1}{x_2-x_1}=\\frac{kx_2+b-kx_1-b}{x_2-x_1}=k\\frac{x_2-x_1}{x_2-x_1}=k$$  \n",
    "\n",
    "Видим, что приращение в точке у прямой не зависит от $x$ и $\\Delta x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcFoC9JpP9A"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Приращение произвольной функции</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNfWm09WpP9A"
   },
   "source": [
    "Но что, если функция не линейная, а произвольная $f(x)$?  \n",
    "В таком случае просто нарисуем **касательную ** в точке, в которой ищем приращение, и будем смотреть уже на приращение касательной. Так как касательная - это прямая, мы уже знаем, какое у неё приращение (см. выше).\n",
    "![source: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/d/d2/Tangent-calculus.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "137KHuBjpP9B"
   },
   "source": [
    "Имея граик функции мы, конечно, можем нарисовать касательную в точке. Но часто функции заданы аналитически, и хочется уметь сразу быстро получать формулу для приращения функциии в точке. Тут на помощь приходит **производная**.  Давайте посмотрим на определение производной его с нашим понятием приращения:  \n",
    "\n",
    "$$f'(x) = \\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x} = \\lim_{\\Delta x \\to 0}\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$  \n",
    "\n",
    "То есть по сути, значение производной функции в точке - это и есть приращение функции, если мы стремим длину отрезка $\\Delta x$ к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7YksIkmlpP9C"
   },
   "source": [
    "Посомтрим на интерактивное демо, демонстрирующее стремление $\\Delta x$ к нулю (*в Google Colab работать не будет!*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 764,
     "status": "error",
     "timestamp": 1540841307892,
     "user": {
      "displayName": "Илья Дмитриевич Захаркин",
      "photoUrl": "",
      "userId": "09157257912804633784"
     },
     "user_tz": -180
    },
    "id": "v9rhGojJpP9D",
    "outputId": "ba6596ca-53ac-45aa-ea53-affe48ac21ac"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4DbxljwpP9F"
   },
   "outputs": [],
   "source": [
    "# pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ_xbrHXpP9I"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b17cf9a06b489b970e3a33e0ca66a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='lg_z', max=4.0, min=-0.5), Output()), _dom_classes=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(lg_z=(-0.5,4.0,0.1))\n",
    "def f(lg_z=1.0):\n",
    "    z = 10 ** lg_z\n",
    "    x_min = 1.5 - 6/z\n",
    "    x_max = 1.5 + 6/z\n",
    "    l_min = 1.5 - 4/z\n",
    "    l_max = 1.5 + 4/z\n",
    "    xstep = (x_max - x_min)/100\n",
    "    lstep = (l_max - l_min)/100\n",
    "    \n",
    "    x = np.arange(x_min, x_max, xstep)\n",
    "    \n",
    "    plt.plot(x, np.sin(x), '-b')     \n",
    "    \n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    plt.plot((l_min,l_max), (np.sin(l_min), np.sin(l_min)), '-r')\n",
    "    plt.plot((l_max,l_max), (np.sin(l_min), np.sin(l_max)), '-r')\n",
    "    \n",
    "    yax = plt.ylim()    \n",
    "    \n",
    "    plt.text(l_max + 0.1/z, (np.sin(l_min) + np.sin(l_max)) / 2, \"$\\Delta y$\")\n",
    "    plt.text((l_min + l_max)/2, np.sin(l_min) - (yax[1]-yax[0]) / 20, \"$\\Delta x$\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print('slope =', (np.sin(l_max) - np.sin(l_min)) / (l_max - l_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8CYa2CRpP9N"
   },
   "source": [
    "Видим, что при уменьшении отрезка $\\Delta x$, значение приращения стабилизируется (перестаёт изменяться). Это число и есть приращение функции в точке, равное проиводной функции в точке. Производную функции $f(x)$ в точке x обознают как $f'(x)$ или как $\\frac{d}{dx}(f(x))$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMwBqnhVpP9N"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Пример вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JwlAAsznpP9P"
   },
   "source": [
    "Возьмём производную по определению:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6R_rnMsqpP9P"
   },
   "source": [
    "1. $f(x)=x$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{x+\\Delta x-x}{\\Delta x}=1\\Rightarrow \\mathbf{\\frac{d}{dx}(x)=1}$$  \n",
    "\n",
    "2. $f(x)=x^2$  \n",
    "\n",
    "$$\\frac{\\Delta y}{\\Delta x}=\\frac{(x+\\Delta x)^2-x^2}{\\Delta x}=\\frac{x^2+2x\\Delta x+\\Delta x^2-x^2}{\\Delta x}=2x+\\Delta x\\rightarrow 2x (\\Delta x\\rightarrow 0)\\Rightarrow \\mathbf{\\frac{d}{dx}(x^2)=2x}$$  \n",
    "    \n",
    "3. В общем случае для степенной функции $f(x)=x^n$ формула будет такой:  \n",
    "\n",
    "$$\\mathbf{\\frac{d}{dx}(x^n)=nx^{n-1}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KP4jUOaqpP9P"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Правила вычисления проиводной</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8fb0go1lpP9Q"
   },
   "source": [
    "Выпишет правила *дифференцирования*:  \n",
    "\n",
    "1). Если $f(x)$ - константа, то её производная (приращение) 0:  \n",
    "\n",
    "$$(C)' = 0$$\n",
    "\n",
    "2). Производная суммы функций - это сумма производных:  \n",
    "\n",
    "$$(f(x) + g(x))' = f'(x) + g'(x)$$\n",
    "\n",
    "3). Производная разности - разность производных:  \n",
    "\n",
    "$$(f(x) - g(x))' = f'(x) - g'(x)$$\n",
    "\n",
    "4). Производная произведения функций:  \n",
    "\n",
    "$$(f(x)g(x))' = f'(x)g(x) + f(x)g'(x)$$\n",
    "\n",
    "5). Производная частного:  \n",
    "\n",
    "$$\\left(\\frac{f(x)}{g(x)}\\right)'=\\frac{f'(x)g(x)-g'(x)f(x)}{g^2(x)}$$\n",
    "\n",
    "6). Производная сложной функции (\"правило цепочки\", \"chain rule\"):  \n",
    "\n",
    "$$(f(g(x)))'=f'(g(x))g'(x)$$\n",
    "\n",
    "Можно записать ещё так:  \n",
    "\n",
    "$$\\frac{d}{dx}(f(g(x)))=\\frac{df}{dg}\\frac{dg}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZFTReW5ppP9R"
   },
   "source": [
    "**Примеры**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grjqr2h4pP9R"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = \\frac{x^2}{cos(x)} + 100$$:  \n",
    "\n",
    "$$f'(x) = \\left(\\frac{x^2}{cos(x)}+100\\right)' = \\left(\\frac{x^2}{cos(x)}\\right)' + (100)' = \\frac{(2x)\\cos(x) - x^2(-\\sin(x))}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TSqCaSSYpP9T"
   },
   "source": [
    "* Вычислим производную функции $$f(x) = tg(x)$$:  \n",
    "\n",
    "$$f'(x) = \\left(tg(x)\\right)' = \\left(\\frac{\\sin(x)}{\\cos(x)}\\right)' = \\frac{\\cos(x)\\cos(x) - \\sin(x)(-\\sin(x))}{cos^2(x)} = \\frac{1}{cos^2(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FXP_YETzpP9T"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Частные производные</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJDwZBCZpP9T"
   },
   "source": [
    "Когда мы имеем функци многих переменных, её уже сложнее представить себе в виде рисунка (в случае более 3-х переменных это действительно не всем дано). ОДнако формальные правила взятия производной у таких функций созраняются. Они в точности совпадают с тоеми, которые рассмотрены выше для функции одной переменной.  \n",
    "\n",
    "Итак, правило взятия частной производной функции мнгих переменных:  \n",
    "1). Пусть $f(\\overline{x}) = f(x_1, x_2, .., x_n)$ - функция многих переменных;  \n",
    "2). Частная проиводная по $x_i$ это функции - это производная по x_i, считая все остальные переменные **константами**. \n",
    "\n",
    "Более математично:  \n",
    "\n",
    "Частная производная функции $f(x_1,x_2,...,x_n)$ по $x_i$ равна  \n",
    "\n",
    "$$\\frac{\\partial f(x_1,x_2,...,x_n)}{\\partial x_i}=\\frac{df_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)}{dx_i}$$  \n",
    "\n",
    "где $f_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)$ означает, что переменные $x_1,...,x_{i-1},x_{i+1},...x_n$ - это фиксированные значения, и с ними нужно обращаться как с константами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3uPIkZ-wpP9U"
   },
   "source": [
    "**Примеры**:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aodpt9VppP9V"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y) = -x^7 + (y - 2)^2 + 140$ по $x$ и по $y$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = -7x^6$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = 2(y - 2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3pRitR-YpP9W"
   },
   "source": [
    "* Найдём частные производные функции $f(x, y, z) = \\sin(x)\\cos(y)tg(z)$ по $x$, по $y$ и по $z$:  \n",
    "\n",
    "$$f_x'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{x}} = \\cos(x)\\cos(y)tg(z)$$  \n",
    "$$f_y'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\sin(x)(-\\sin(y))tg(z)$$\n",
    "$$f_z'(x, y) = \\frac{\\partial{f(x, y)}}{\\partial{y}} = \\frac{\\sin(x)\\cos(y)}{\\cos^2{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nrmPlYyrpP9X"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Градиентный спуск</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeD8U4CApP9X"
   },
   "source": [
    "**Градиентом** функции $f(\\overline{x})$, где $\\overline{x} \\in \\mathbb{R^n}$, то есть $\\overline{x} = (x_1, x_2, .., x_n)$, называется вектор из частных производных функции $f(\\overline{x})$:  \n",
    "\n",
    "$$grad(f) = \\nabla f(\\overline{x}) = \\left(\\frac{\\partial{f(\\overline{x})}}{\\partial{x_1}}, \\frac{\\partial{f(\\overline{x})}}{\\partial{x_2}}, .., \\frac{\\partial{f(\\overline{x})}}{\\partial{x_n}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcjYCHOepP9Y"
   },
   "source": [
    "Есть функция $f(x)$. Хотим найти аргумент, при котором она даёт минимум.\n",
    "\n",
    "Алгоритм градиентного спуска:  \n",
    "1. $x^0$ - начальное значение (обычно берётся просто из разумных соображений или случайное);  \n",
    "2. $x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1})$, где $\\nabla f(x^{i-1})$ - это градиент функции $f$, в который подставлено значение $x^{i-1}$;\n",
    "3. Выполнять пункт 2, пока не выполнится условие остановки: $||x^{i} - x^{i-1}|| < eps$, где $||x^{i} - x^{i-1}|| = \\sqrt{(x_1^i - x_1^{i-1})^2 + .. + (x_n^i - x_n^{i-1})^2}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zX1miuQ0pP9Z"
   },
   "source": [
    "**Примеры:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1M6agxdpP9Z"
   },
   "source": [
    "* *Пример 1*: Посчитаем формулу градиентного спуска для функции $f(x) = 10x^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WMJRqDRpP9a"
   },
   "source": [
    "$x^i = x^{i-1} - \\alpha \\nabla f(x^{i-1}) = x^{i-1} - \\alpha f'(x^{i-1}) = x^{i-1} - \\alpha (20x^{i-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EqjopRZVpP9b"
   },
   "source": [
    "Имея эту формулу, напишем код градиентного спуска для функции $f(x) = 10x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evLahkyIpP9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x**2\n",
    "\n",
    "def gradient_descent(alpha=0.001, eps=0.01):\n",
    "    x_pred = 100  # начальная инициализация\n",
    "    x = 50  # начальная инициализация\n",
    "    for _ in range(100000):\n",
    "        print(_)  # смотрим, на каком мы шаге\n",
    "        if np.sum((x - x_pred)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_pred = x\n",
    "        x = x_pred - 20 * alpha * x_pred  # по формуле выше\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1539764935297,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "9k8A7ei8pP9g",
    "outputId": "7d0401a9-5810-4215-89af-4a9d077c05eb"
   },
   "outputs": [],
   "source": [
    "# x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1539764937543,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "uGOVAybRpP9k",
    "outputId": "2c6d1f12-ce1e-491f-ca98-6e1fe98849c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48948719814064656"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 598,
     "status": "ok",
     "timestamp": 1539764938543,
     "user": {
      "displayName": "Григорий Лелейтнер",
      "photoUrl": "",
      "userId": "07179937308049589303"
     },
     "user_tz": -180
    },
    "id": "_EW8AY8VpP9n",
    "outputId": "4d7b79da-706d-4d04-c1ba-29d39625760f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.395977171435806"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tsqfsTezpP9q"
   },
   "source": [
    "* *Пример 2*: Посчитаем формулу градиентного спуска для функции $f(x, y) = 10x^2 + y^2$:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNWMBcbNpP9r"
   },
   "source": [
    "$$\\left(\\begin{matrix} x^i \\\\ y^i \\end{matrix}\\right) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\nabla f(x^{i-1}, y^{i-1}) = \\left(\\begin{matrix} x^{i-1} \\\\ y^{i-1} \\end{matrix}\\right) - \\alpha \\left(\\begin{matrix} \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{x}} \\\\ \\frac{\\partial{f(x^{i-1}, y^{i-1})}}{\\partial{y}} \\end{matrix}\\right) = x^{i-1} - \\alpha \\left(\\begin{matrix} 20x^{i-1} \\\\ 2y^{i-1} \\end{matrix}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sBnijsKLpP9r"
   },
   "source": [
    "Осталось написать код, выполняющий градиентный спуск, пока не выполнится условие остановки, для функции $f(x, y) = 10x^2 + y^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_rDsja-pP9s"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 10 * x[0]**2 + x[1]**2\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.001):\n",
    "    x_prev = np.array([100, 100])  # начальная инициализация\n",
    "    x = np.array([50, 50])  # начальная инициализация\n",
    "    for _ in (range(100000)):\n",
    "        print(_)  # смотрим, на каком мы шаге\n",
    "        if np.sum((x - x_prev)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_prev = x\n",
    "        x = x_prev - alpha * np.array(20 * x_prev[0], 2 * x_prev[1])  # по формуле выше\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boueQCnXpP9u"
   },
   "outputs": [],
   "source": [
    "# x_min = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pyhQsmXpP9x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00272226, 0.00272226])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytAfn_X7pP90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.151763082307056e-05"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyKaCWuJpP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Домашнее задание</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jrjiC9mUpP93"
   },
   "source": [
    "1). (только для тех, кто раньше брал производные) Вычислите производную функции $f(x)=\\frac{1}{x}$ по определению и сравните с производной степенной функции в общем случае;  \n",
    "2). Найдите производную функции $Cf(x)$, где С - число;  \n",
    "3). Найдите производные функций:  \n",
    "\n",
    "$$f(x)=x^3+3\\sqrt{x}-e^x$$\n",
    "\n",
    "$$f(x)=\\frac{x^2-1}{x^2+1}$$\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "$$L(y, \\hat{y}) = (y-\\hat{y})^2$$  \n",
    "\n",
    "4). Напишите формулу и код для градиентного спуска для функции:  \n",
    "$$f(w, x) = \\frac{1}{1 + e^{-wx}}$$  \n",
    "\n",
    "То есть по аналогии с примером 2 вычислите частные производные по $w$ и по $x$ и запишите формулу векторно (см. пример 2)\n",
    "\n",
    "В задаче 3 производную нужно брать по $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50. 50.]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x_prev):\n",
    "    return 1 / (1 + np.exp(-1 * x_prev))\n",
    "\n",
    "\n",
    "def gradient_descent(alpha=0.01, eps=0.0000001):\n",
    "    x_prev = np.array([100, 100])  # начальная инициализация\n",
    "    x = np.array([50, 50])  # начальная инициализация\n",
    "    for _ in (range(100000)):\n",
    "        # print(_)  # смотрим, на каком мы шаге\n",
    "        if np.sum((x - x_prev)**2) < eps**2:  # условие остановки\n",
    "            break\n",
    "        x_prev = x\n",
    "        \n",
    "        x = x_prev - alpha * f(x_prev) * (1 - f(x_prev))  # по формуле выше\n",
    "    return x\n",
    "\n",
    "x_min = gradient_descent()\n",
    "\n",
    "print(x_min)\n",
    "print(f([50, 50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxDBOB04pP93"
   },
   "source": [
    "<h3 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nm0VC825pP95"
   },
   "source": [
    "0). Прикольный сайт с рисунками путём задания кривых уравнениями и функциями:  \n",
    "\n",
    "https://www.desmos.com/calculator/jwshvscdzb\n",
    "\n",
    "***Производные:***\n",
    "\n",
    "1). Про то, как брать частные производные:  \n",
    "\n",
    "http://www.mathprofi.ru/chastnye_proizvodnye_primery.html\n",
    "\n",
    "2). Сайт на английском, но там много видеоуроков и задач по производным:  \n",
    "\n",
    "https://www.khanacademy.org/math/differential-calculus/derivative-intro-dc\n",
    "\n",
    "3). Задачи на частные производные:  \n",
    "\n",
    "http://ru.solverbook.com/primery-reshenij/primery-resheniya-chastnyx-proizvodnyx/  \n",
    "\n",
    "4). Ещё задачи на частные проивзодные:  \n",
    "\n",
    "https://xn--24-6kcaa2awqnc8dd.xn--p1ai/chastnye-proizvodnye-funkcii.html  \n",
    "\n",
    "5). Производные по матрицам:  \n",
    "\n",
    "http://nabatchikov.com/blog/view/matrix_der  \n",
    "\n",
    "***Градиентны спуск:***\n",
    "\n",
    "6). [Основная статья по градиентному спуску](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)\n",
    "\n",
    "7). Статья на Хабре про градиетный спуск для нейросетей:  \n",
    "\n",
    "https://habr.com/post/307312/  \n",
    "\n",
    "***Методы оптимизации в нейронных сетях:***\n",
    "\n",
    "8). Сайт с анимациями того, как сходятся алгоритмы градиентного спуска:\n",
    "www.denizyuret.com/2015/03/alec-radfords-animations-for.html\n",
    "\n",
    "9). Статья на Хабре про метопты (град. спуск) в нейронках:\n",
    "https://habr.com/post/318970/\n",
    "\n",
    "10). Ещё сайт (англ.) про метопты (град. спуск) в нейронках (очень подробно):\n",
    "http://ruder.io/optimizing-gradient-descent/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[seminar]derivative_gradient.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
